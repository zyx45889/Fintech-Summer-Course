## 实验 1：金融数据获取实验报告

### Python环境安装

课程前已完成python3.7的安装与环境配置

### IDE环境配置

课程前已完成pycharm安装

#### 遇到的问题

pycharm于去年暑假小学期（课程综合实践Ⅰ）安装，到现在正好一年，学生一年免费已到期

#### 解决方法

更新学生证明失败，Jetbrains认为zju.edu.cn邮箱无效；搜索得知最近教育邮箱更新学生证明都遇到问题。尝试通过github学生包，一小时后未收到邮件，怀疑未通过机器审核，需要等待人工审核。由于时间关系搜索了序列码解决问题。

### Scrapy框架安装

通过pycharm直接安装

![image-20200714165456073](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714165456073.png)

![image-20200714165731102](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714165731102.png)

可以看到安装scrapy的同时安装了需要的cryptography，lxml等其他包。

### 爬虫Demo编写

在terminal运行“scrapy startproject tutorial”语句，在spider目录下新建quotes_spider.py

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

在tutorial目录（和cfg文件同目录）新建mian.py

```python
from scrapy.cmdline import execute
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
execute(["scrapy", "crawl", "quotes"])
```

运行main，可以看到运行结果和创建的两个文件

![image-20200714162814128](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714162814128.png)

在terminal按照教程测试抓取结果

![image-20200714162855514](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714162855514.png)

![image-20200714162915369](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714162915369.png)

![image-20200714163236652](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714163236652.png)

![image-20200714163338512](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714163338512.png)

![image-20200714164114991](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714164114991.png)

![image-20200714164300050](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714164300050.png)

![image-20200714164503336](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714164503336.png)

更改quotes_spider.py内容

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }
```

更改main.py内容：

```python
from scrapy.cmdline import execute
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
execute(["scrapy", "crawl", "quotes", "-o", "quotes.json"])
```

可以看到运行结果和新建立的json文件

![image-20200714164723903](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714164723903.png)

![image-20200714164739917](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714164739917.png)

#### 遇到的问题

运行main程序报错：no active project

​									Unknown command: crawl

#### 问题分析

通过查找资料发现应该是根目录问题，尝试：

##### 1.cmd运行

显示scrapy不是可以运行的指令

##### 2.设置根目录

没有改变

##### 3.在pycharm内为main配置config文件

报错变为找不到quotes文件

#### 解决办法

考虑到查找的资料过多，尝试的方法可能互相影响造成了更大的混乱；在建立工程的过程中可能也发生了错误。考虑重装pycharm，重新建立工程，最终顺利解决了问题。

【不过最后思考了一下认为是第一次建工程可能没注意quotes_spider建立的目录位置。XD】

### bonus：抓取网贷之家信息存入mysql数据库

按照上述方法获取网贷之家-数据网页（“https://shuju.wdzj.com/”）html信息，用chorm打开

![image-20200714215310024](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714215310024.png)

在右边界面选择要抓取的内容（公司名称以及其四个相关数据），右键选择复制选择器，得到css selector表达式，根据得到的表达式编写爬虫代码：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'https://shuju.wdzj.com/',
    ]

    def parse(self, response):
        for company in response.css('#platTable > tr'):
            yield {
                'name': company.css('td:nth-child(8) > div::attr(data-platname)').get(),
                'money': company.css('td:nth-child(3) > div::text').get(),
                'ben':company.css('td:nth-child(4) > div::text').get(),
                'time':company.css('td:nth-child(5) > div::text').get(),
                'waiting': company.css('td:nth-child(6) > div::text').get(),
            }
```

试着讲读到的数据存入json文件，检查前半部分工作是否正确，从下图json文件截图可以看出，虽然由于utf-8显示不出部分中文字符，但是对照网页可以确认抓取信息正确。

![image-20200714215540933](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714215540933.png)

更改pipline代码，连接数据库

```python
import pymysql


class TutorialPipeline(object):
    def __init__(self):
        # connection database
        self.connect = pymysql.connect("localhost","root","jkwry4s45889","wdzj" )
        # get cursor
        self.cursor = self.connect.cursor()
        print("连接数据库成功")

    def process_item(self, item, spider):
        # sql语句
        insert_sql = """
        insert into company(name, money, ben, time, waiting) VALUES (%s,%s,%s,%s,%s)
        """
        # 执行插入数据到数据库操作
        self.cursor.execute(insert_sql, (item['name'], item['money'], item['ben'], item['time'],
                                         item['waiting']))
        # 提交，不进行提交无法保存到数据库
        self.connect.commit()

    def close_spider(self, spider):
        # 关闭游标和连接
        self.cursor.close()
        self.connect.close()
```

更改setting代码：

```python
BOT_NAME = 'tutorial'

SPIDER_MODULES = ['tutorial.spiders']
NEWSPIDER_MODULE = 'tutorial.spiders'

ITEM_PIPELINES = {
    'pipelines.TutorialPipeline': 200,
}

ROBOTSTXT_OBEY = True
USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
```

在数据库中建立新的数据库和表格，运行爬虫程序。

在mysql中查询表格内容，确认抓取成功

![image-20200714215834515](C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20200714215834515.png)

#### 遇到的问题

1.网贷之家网站设置了反爬虫，无法直接爬取

2.mysql远程连接失败，显示localhost没有权限连接

#### 解决方法

1.在setting中增加“USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'”语句

2.多次restart mysql